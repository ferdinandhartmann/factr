# Copyright (c) Sudeep Dasari, 2023

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

defaults:
  - agent: transformer_vit
  - task: franka
  - trainer: mae_cos_sched
  - _self_

hydra:
  run:
    dir: checkpoints/${exp_name}

rt: ${hydra:runtime.choices.agent/features}

exp_base_name: 20251112_60_25hz_filt2_3dof
# exp_name: 20251107_60_25hz_s40_ac25_b64_lr28_10
batch_size: 64
num_workers: 8
lr: 0.0002
max_iterations: 5000 # 20/50 epochs good maybe, (batch_size*max_iterations) / dataset_size , datasetsize=(54 episodes Ã— (300 raw steps)=16,200)
eval_freq: 200
save_freq: 2000
schedule_freq: 1
devices: 1
seed: 42

suffix: _7torque

# dataset_size = number_of_episodes * number_of_timesteps 
#              = 54 * 300 = 16200
#       epochs = (batch_size * max_iterations) / dataset_size
#              = (64 * 10000) / 16200 = 39.5

buffer_path: ./buffer.pkl
# buffer_path: /home/ferdinand/factr/process_data/processed_data/20251024_train_60_filtered/buf.pkl
ac_chunk: 25
img_chunk: 1  # number of image timesteps to use (including current one)
train_transform: medium
test_transform: preproc

curriculum:
  space: pixel # pixel, latent
  operator: blur # blur, downsample
  scheduler: linear # no, const, linear, step, exp, cos
  start_scale: 5
  stop_scale: 0
  max_step: ${max_iterations}

use_indices: [1,3,5] 

exp_name: ${exp_base_name}_s${seed}_ac${ac_chunk}_b${batch_size}_lr${lr}_iter${max_iterations}${suffix}

wandb:
  name: ${exp_name}
  project: factr
  group: bc
  debug: False
  entity: None