# Copyright (c) Sudeep Dasari, 2023

# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.


defaults:
  - agent: transformer_vit
  - task: franka
  - trainer: mae_cos_sched
  - _self_

hydra:
  run:
    dir: checkpoints/${exp_name}

rt: ${hydra:runtime.choices.agent/features}

exp_name: 20251024_60_25hz_b64_lr27_d12_7_latent_cos_2
batch_size: 64
num_workers: 8
lr: 0.00027
max_iterations: 7000 # 20/50 epochs good mazbe, (batch_size*max_iterations) / dataset_size , datasetsize=(54 episodes Ã— (300 raw steps)=16,200)
eval_freq: 250
save_freq: 2500
schedule_freq: 1
devices: 1
seed: 42

# dataset_size = number_of_episodes * number_of_timesteps 
#              = 54 * 300 = 16200
#       epochs = (batch_size * max_iterations) / dataset_size
#              = (64 * 10000) / 16200 = 39.5


buffer_path: ./buffer.pkl
# buffer_path: /home/ferdinand/factr/process_data/processed_data/20251024_train_60_filtered/buf.pkl
ac_chunk: 25
img_chunk: 1  # number of image timesteps to use (including current one)
train_transform: medium
test_transform: preproc

curriculum:
  space: latent # pixel, latent
  operator: blur # blur, downsample
  scheduler: cos # no, const, linear, step, exp, cos
  start_scale: 5
  stop_scale: 0
  max_step: ${max_iterations}

wandb:
  name: ${exp_name}
  project: factr
  group: bc
  debug: False
  entity: None